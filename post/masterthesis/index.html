<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Master Thesis: High Availability in Lifecycle Management of Cloud-Native Network Functions | Ziheng Zhang's Blog</title>
<meta name=keywords content="Kubernetes,Docker,Golang"><meta name=description content="A Near-Zero Downtime Database Version Change Prototype"><meta name=author content="Ziheng Zhang"><link rel=canonical href=https://zzheng2020.github.io/post/masterthesis/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://zzheng2020.github.io/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://zzheng2020.github.io/favicon/favicon.ico><link rel=icon type=image/png sizes=32x32 href=https://zzheng2020.github.io/favicon/favicon.ico><link rel=apple-touch-icon href=https://zzheng2020.github.io/favicon/apple-touch-icon.png><link rel=mask-icon href=https://zzheng2020.github.io/favicon/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://zzheng2020.github.io/post/masterthesis/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Master Thesis: High Availability in Lifecycle Management of Cloud-Native Network Functions"><meta property="og:description" content="A Near-Zero Downtime Database Version Change Prototype"><meta property="og:type" content="article"><meta property="og:url" content="https://zzheng2020.github.io/post/masterthesis/"><meta property="og:image" content="https://github.com/user-attachments/assets/ee6aca7f-a52f-4602-a3e1-4d2fbcfb24bb"><meta property="article:section" content="post"><meta property="article:published_time" content="2023-08-01T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-01T00:00:00+00:00"><meta property="og:site_name" content="Ziheng Zhang's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://github.com/user-attachments/assets/ee6aca7f-a52f-4602-a3e1-4d2fbcfb24bb"><meta name=twitter:title content="Master Thesis: High Availability in Lifecycle Management of Cloud-Native Network Functions"><meta name=twitter:description content="A Near-Zero Downtime Database Version Change Prototype"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://zzheng2020.github.io/post/"},{"@type":"ListItem","position":2,"name":"Master Thesis: High Availability in Lifecycle Management of Cloud-Native Network Functions","item":"https://zzheng2020.github.io/post/masterthesis/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Master Thesis: High Availability in Lifecycle Management of Cloud-Native Network Functions","name":"Master Thesis: High Availability in Lifecycle Management of Cloud-Native Network Functions","description":"A Near-Zero Downtime Database Version Change Prototype","keywords":["Kubernetes","Docker","Golang"],"articleBody":"This blog introduces my master’s thesis ([pdf] [presentation] [code]) and the demo video can be found below. Most of the sections correspond to paragraphs from the thesis, while some parts that are necessary in the thesis but not needed in the blog have been omitted. This allows you to focus more on the project itself.\nBackground In any infrastructure, be it clustered or otherwise, databases often face challenges with version changes, including upgrades and downgrades. Upgrades are straightforward, driven by the need to adopt new features or improvements. However, downgrades, especially in the telecommunications industry, present unique challenges. For example, in the Access and Mobility Management Function (AMF) as shown in the image below, when a database is upgraded and data is converted to a new version, complications may arise. According to ETSI MANO specifications, a rollback to the previous version may be required in such cases.\n3GPP 5G Core Architecture Another scenario is the “observation period” following an upgrade. For example, after upgrading a service from version A to version B, its performance is monitored. Key questions arise: Is the service functioning correctly? Is it handling the expected load? If the service underperforms, a rollback may be required, presenting the same challenges as in the first scenario. Both cases highlight the need for better support for database downgrades to mitigate service disruptions and maintain user experience.\nKubernetes Operator A Kubernetes Operator (K8s Operator) encodes human operational expertise into software. The image below illustrates the key differences between using a K8s Operator and not.\nComparison of Cluster Management: Manual vs. K8s Operator Without a K8s Operator, maintainers must deeply understand how stateful applications work within the cluster, execute commands in the correct order, and manually verify responses.\nWith a K8s Operator, maintainers can perform complex operations, like database upgrades or downgrades, by simply modifying the custom resource definition (YAML file), without needing to understand the internal mechanisms.\nIn these scenarios, the K8s Operator acts like an experienced maintainer, monitoring the cluster 24/7 and exposing a simple API, greatly reducing the chance of human errors.\nZalando Postgres Operator The Zalando Postgres Operator is a widely-used tool in the industry, known for supporting both minor and major version upgrades.\nMinor upgrades, such as moving from version 1.0 to 1.1, are straightforward and can be performed without downtime. However, major upgrades, like transitioning from version 1.0 to 2.0, present more challenges.\nThe Zalando Operator offers two upgrade methods:\nUpgrade via Cloning: This approach requires the new cluster to run a higher version than the source. It may involve significant downtime, as all write operations must stop, and Write-Ahead Log (WAL) files must be archived before cloning begins. In-place Major Version Upgrade: While more convenient, this method is irreversible once the pg_upgrade command is executed. The Zalando Postgres Operator primarily focuses on upgrades but may involve significant downtime. The search for a more flexible solution, especially with downgrade support and reduced downtime, remains ongoing.\nConventional Strategy The conventional strategy involves the following steps:\nStop the service to halt user requests. Back up the database to be upgraded. Set up the target version database and migrate the backup data. Verify data consistency between the original and target databases. If verification succeeds, resume the service and accept user requests. Conventional Strategy While this approach effectively handles database version changes, it requires system downtime, which depends on the speed of data migration and validation—the faster these processes, the shorter the downtime.\nHowever, modern businesses increasingly demand high availability alongside system stability. For example, achieving “five nines” (99.999% uptime) requires minimal downtime, as shown in table below. The conventional strategy falls short of meeting these strict requirements, highlighting the need for alternative solutions that mitigate downtime and maintain high availability during database upgrades.\nAvailability % Downtime per year Downtime per day (24 hours) 99% 3.65 days 14.40 minutes 99.99% 52.6 minutes 8.64 seconds 99.999% 5.26 minutes 864 milliseconds Our Solution To address the challenges of database version changes and meet system design goals, the blue-green deployment strategy offers a practical solution.\nThe blue-green deployment strategy uses two parallel environments, “blue” and “green”. Each environment hosts a different database version, enabling smooth transitions between versions with near-zero downtime. The key benefit is continuous system operation during the version change, significantly reducing downtime.\nThe strategy involves four main steps as shown in the below image:\nPreparation: Set up both environments—blue hosting the current version and green the target version. The green environment is tested to ensure readiness for the transition.\nData Synchronization: Synchronize data between the blue and green environments to ensure consistency. This process is automated, reducing manual effort and enhancing efficiency and reliability.\nSwitching: After synchronization, the system switches from the blue to the green environment seamlessly, ensuring no service disruption, thus maintaining client transparency.\nMonitoring: Post-switch, the new database version is monitored for performance and consistency. If issues arise, the system can quickly revert to the blue environment to minimize client impact.\nBlue-Green Strategy By using the blue-green deployment strategy, we can efficiently manage database version changes, automate processes, and maintain client transparency, overcoming the limitations of conventional methods and better supporting customers’ needs.\nAchieving Synchronisation between The Two PostgreSQL Databases In the blue-green deployment strategy, synchronizing the two PostgreSQL databases is essential for maintaining data consistency and ensuring client transparency during version changes. We use logical replication as the primary method for database synchronization. It allows for selective, real-time data replication from one database to another. It ensures that changes in the source database are immediately reflected in the target database. This method is ideal for blue-green deployments, enabling smooth transitions between database versions with near-zero downtime and client disruption.\nMonitoring Synchronisation Progress A critical factor in ensuring a smooth database version change is monitoring the synchronization progress between the master and follower nodes. In PostgreSQL, the pg_stat_replication view provides key replication status metrics to track synchronization completion.\nThe following metrics from the pg_stat_replication view are essential for monitoring synchronization progress:\nsent_lsn: Represents the latest Write-Ahead Log (WAL) position sent by the master to the follower. pg_current_wal_flush_lsn: Returns the latest WAL position flushed to disk on the master. pg_wal_lsn_diff(lsn1, lsn2): Calculates the byte difference between two WAL positions. Determining Synchronisation Completion To determine when synchronization is complete, we compare the sent_lsn value with the pg_current_wal_flush_lsn value. When they match, it indicates that the latest WAL position sent by the master has been flushed to disk, marking the completion of synchronization.\nAdditionally, the pg_wal_lsn_diff function can track the remaining byte difference between the master and follower WAL positions, helping estimate the remaining synchronization time for more accurate system adjustments.\nBy utilizing the pg_stat_replication view and associated functions, we can effectively monitor synchronization progress, ensuring near-zero downtime, automation, and transparent operation during database version changes.\nIn practice, since clients may continuously write to the database, sent_lsn may never fully catch up with pg_current_wal_flush_lsn. However, the difference is typically small. To ensure synchronization, we temporarily halt write operations and wait for the values to match. This is why we refer to it as “near-zero” downtime rather than “zero” downtime.\nIntegrating Blue-Green Deployment Strategy with Kubernetes Custom Resource Definitions (CRDs) extend Kubernetes API, allowing users to define and manage custom resources.\nTo design the CRD, we define a schema with the following key fields:\napiVersion: The API version used for the CRD. kind: The type of Kubernetes object, in this case, a custom resource for blue-green deployment. metadata: Includes details like name, namespace, labels, and annotations. spec: Specifies the properties of the custom resource. Implementation We begin by discussing the database synchronization process and the configuration of master and follower nodes. The section then introduces the PgUpgradeReconciler structure and its integration with Kubernetes. The core of the implementation is the reconciliation process, which includes controller setup, deployment creation, service creation, schema synchronization, subscription management, Nginx proxy updates, resource cleanup, and supporting functions. Each step is carefully detailed to ensure smooth integration of the Blue-Green deployment strategy with Kubernetes, enabling database version changes with minimal downtime. This section serves as a practical guide to implementing this solution in real-world scenarios.\nSynchronisation between Databases Both the master and follower nodes require specific configurations.\nMaster Node Configuration The Kubernetes Operator automates these tasks to set up logical replication on the master node:\nConfigure PostgreSQL: Updates postgresql.conf to enable logical replication by setting wal_level to logical. Create Publication: Uses the SQL command CREATE PUBLICATION to create a publication that includes all tables. Export Schema: Runs the pg_dump command to export the database schema to an SQL file. Follower Node Configuration The Kubernetes Operator automates schema synchronization and subscription creation on the follower node:\nSynchronize Schema: Imports the schema from the master using the psql command. Create Subscription: Uses the SQL command CREATE SUBSCRIPTION to establish a subscription to the master node’s publication. PgUpgradeReconciler Structure The PgUpgradeReconciler struct includes the following fields:\nclient.Client: Manages CRUD operations on Kubernetes objects. Scheme: A runtime.Scheme for converting between Go structs and GroupVersionKinds. Log: A logr.Logger instance for logging messages at various verbosity levels. Kubernetes RBAC Kubebuilder annotations define the required RBAC permissions for the controller to manage resources:\npgupgrades: Custom resource for managing PostgreSQL upgrades. pgupgrades/status: Subresource for handling PgUpgrade object statuses. pgupgrades/finalizers: Subresource for managing finalizers on PgUpgrade objects. pods, deployments, services, configmaps: Standard Kubernetes resources for creating, managing, and deleting related resources. Reconciliation Process The Reconcile function is the heart of the controller, managing the reconciliation process. It starts by retrieving the PgUpgrade instance. If the instance is not found, it returns a nil error, allowing the controller to proceed with other instances.\nController Setup The PgUpgradeReconciler struct defines the controller for the PgUpgrade system. The SetupWithManager function sets up the controller with the Manager, associating it with the pgupgradev1.PgUpgrade resource.\nDeployment Creation The function checks if the PgUpgrade deployment exists. If not, it creates one using the deploymentForPgUpgrade function and logs the process. If it exists, it checks if the image matches the one in the PgUpgrade object and updates it if needed.\nService Creation The function checks if the PgUpgrade service exists. If not, it creates one using the serviceForPgUpgrade function and logs it. If the service already exists, it proceeds.\nSchema Synchronization The syncSchema function runs schema synchronization on the target PostgreSQL instance by connecting to the target pod, retrieving its IP, and executing the schema sync command via the remotecommand package.\nSubscription Creation The createSubscriptions function sets up logical replication between the old and new PostgreSQL instances by connecting to the new instance and running the CREATE SUBSCRIPTION command with the necessary parameters.\nNginx Proxy Update The changeNginxProxyPass function updates the Nginx configuration stored in a ConfigMap. It retrieves the ConfigMap, modifies the proxy_pass directive to point to the new follower’s IP, and updates the ConfigMap in the cluster.\nResource Deletion The deleteResource function removes specified resources (e.g., deployments) from the cluster. It iterates over the resource names in the KillDeployments field of the PgUpgrade resource and deletes them, logging the process.\nHelper Functions Various helper functions support the system, including:\nlabelsForPgUpgrade: Generates labels for a PgUpgrade resource. getFollowerIP: Retrieves the follower’s IP from the cluster by fetching the pod’s IP based on the OldDBLabel. Prototype Outcomes The demo video above clearly shows the process and the outcome of this project. Here I would like to discuss the performance of our solution.\nPerformance Analysis We use PgAdmin4 to analyze the performance metrics of the database within the Kubernetes cluster. These metrics include database sessions, transactions per second, tuples in/out, and block I/O.\nFigure PA-1 shows the baseline database performance, with stable database sessions and tuples in. Transactions per second, tuples out, and block I/O follow a consistent and regular pattern.\nPA-1 The initial performance metrics. Figure PA-2 depicts the system’s behavior after starting the client program, which runs five goroutines. Each goroutine reads from the database every three seconds and writes every ten seconds, reflecting the typical pattern of more frequent reads than writes. This activity causes a noticeable increase in all metrics, but the system remains stable.\nPA-2 The performance metrics after running the client. Figure PA-3 illustrates the system’s response when our Kubernetes operator initiates a database version change. Aside from a minor increase in the “tuples in” metric (due to client operations), a brief fluctuation in metrics is observed, but the system quickly returns to normal. These rapid changes, highlighted by red boxes, are expected as the version change involves data synchronization between the master and follower nodes. Importantly, no idle sessions or transactions occur during this process, demonstrating that our solution achieves near-zero downtime while remaining transparent to clients.\nPA-3 The performance metrics after deploying the Kubernetes operator. The results confirm that our solution effectively handles database version migrations and traffic switching. The smooth user experience during the migration highlights the approach’s practicality and reliability.\n","wordCount":"2137","inLanguage":"en","image":"https://github.com/user-attachments/assets/ee6aca7f-a52f-4602-a3e1-4d2fbcfb24bb","datePublished":"2023-08-01T00:00:00Z","dateModified":"2023-08-01T00:00:00Z","author":[{"@type":"Person","name":"Ziheng Zhang"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://zzheng2020.github.io/post/masterthesis/"},"publisher":{"@type":"Organization","name":"Ziheng Zhang's Blog","logo":{"@type":"ImageObject","url":"https://zzheng2020.github.io/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://zzheng2020.github.io/ accesskey=h title="Ziheng (Alt + H)"><img src=https://zzheng2020.github.io/favicon/apple-touch-icon.png alt aria-label=logo height=35>Ziheng</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://zzheng2020.github.io/post/ title=Posts><span>Posts</span></a></li><li><a href=https://zzheng2020.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://zzheng2020.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://zzheng2020.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://zzheng2020.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://zzheng2020.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://zzheng2020.github.io/post/>Posts</a></div><h1 class="post-title entry-hint-parent">Master Thesis: High Availability in Lifecycle Management of Cloud-Native Network Functions</h1><div class=post-description>A Near-Zero Downtime Database Version Change Prototype</div><div class=post-meta><span title='2023-08-01 00:00:00 +0000 UTC'>August 1, 2023</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;2137 words&nbsp;·&nbsp;Ziheng Zhang&nbsp;|&nbsp;<a href=https://github.com/zzheng2020/zzheng2020.github.io/tree/main/content/post/MasterThesis.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><figure class=entry-cover><img loading=eager src=https://github.com/user-attachments/assets/ee6aca7f-a52f-4602-a3e1-4d2fbcfb24bb alt></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#background>Background</a></li><li><a href=#kubernetes-operator>Kubernetes Operator</a><ul><li><a href=#zalando-postgres-operator>Zalando Postgres Operator</a></li></ul></li><li><a href=#conventional-strategy>Conventional Strategy</a></li><li><a href=#our-solution>Our Solution</a><ul><li><a href=#achieving-synchronisation-between-the-two-postgresql-databases>Achieving Synchronisation between The Two PostgreSQL Databases</a></li><li><a href=#monitoring-synchronisation-progress>Monitoring Synchronisation Progress</a></li><li><a href=#integrating-blue-green-deployment-strategy-with-kubernetes>Integrating Blue-Green Deployment Strategy with Kubernetes</a></li></ul></li><li><a href=#implementation>Implementation</a><ul><li><a href=#synchronisation-between-databases>Synchronisation between Databases</a></li><li><a href=#pgupgradereconciler-structure><code>PgUpgradeReconciler</code> Structure</a></li><li><a href=#kubernetes-rbac>Kubernetes RBAC</a></li><li><a href=#reconciliation-process>Reconciliation Process</a></li></ul></li><li><a href=#prototype-outcomes>Prototype Outcomes</a><ul><li><a href=#performance-analysis>Performance Analysis</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>This blog introduces my master&rsquo;s thesis (<a href="https://kth.diva-portal.org/smash/record.jsf?pid=diva2%3A1781462&amp;dswid=-7118">[pdf]</a> <a href="https://drive.google.com/file/d/1JTO0_wDDCrYbJaCS6PHPoJIcsNHOAGO2/view?usp=sharing">[presentation]</a> <a href=https://github.com/zzheng2020/Master-Thesis>[code]</a>) and the demo video can be found below. Most of the sections correspond to paragraphs from the thesis, while some parts that are necessary in the thesis but not needed in the blog have been omitted. This allows you to focus more on the project itself.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/L9WOx1jLnF0?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="YouTube video"></iframe></div><h2 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h2><p>In any infrastructure, be it clustered or otherwise, databases often face challenges with version changes, including upgrades and downgrades. Upgrades are straightforward, driven by the need to adopt new features or improvements. However, downgrades, especially in the telecommunications industry, present unique challenges. For example, in the Access and Mobility Management Function (AMF) as shown in the image below, when a database is upgraded and data is converted to a new version, complications may arise. According to ETSI MANO specifications, a rollback to the previous version may be required in such cases.</p><center><img style="zoom:50%;border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" src=https://github.com/user-attachments/assets/9d7f0aa5-16b3-4da7-a8f6-1c4880c3ec6a><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px;margin-top:2px">3GPP 5G Core Architecture</div></center><p>Another scenario is the &ldquo;observation period&rdquo; following an upgrade. For example, after upgrading a service from version A to version B, its performance is monitored. Key questions arise: Is the service functioning correctly? Is it handling the expected load? If the service underperforms, a rollback may be required, presenting the same challenges as in the first scenario. Both cases highlight the need for better support for database downgrades to mitigate service disruptions and maintain user experience.</p><h2 id=kubernetes-operator>Kubernetes Operator<a hidden class=anchor aria-hidden=true href=#kubernetes-operator>#</a></h2><p>A Kubernetes Operator (K8s Operator) encodes human operational expertise into software. The image below illustrates the key differences between using a K8s Operator and not.</p><center><img style="zoom:50%;border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" src=https://github.com/user-attachments/assets/4ce9297a-fbbd-4823-b67b-4af5bfc0ff7c><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px;margin-top:2px">Comparison of Cluster Management: Manual vs. K8s Operator</div></center><p>Without a K8s Operator, maintainers must deeply understand how stateful applications work within the cluster, execute commands in the correct order, and manually verify responses.</p><p>With a K8s Operator, maintainers can perform complex operations, like database upgrades or downgrades, by simply modifying the custom resource definition (YAML file), without needing to understand the internal mechanisms.</p><p>In these scenarios, the K8s Operator acts like an experienced maintainer, monitoring the cluster 24/7 and exposing a simple API, greatly reducing the chance of human errors.</p><h3 id=zalando-postgres-operator>Zalando Postgres Operator<a hidden class=anchor aria-hidden=true href=#zalando-postgres-operator>#</a></h3><p><a href=https://github.com/zalando/postgres-operator/tree/master>The Zalando Postgres Operator</a> is a widely-used tool in the industry, known for supporting both minor and major version upgrades.</p><p>Minor upgrades, such as moving from version 1.0 to 1.1, are straightforward and can be performed without downtime. However, major upgrades, like transitioning from version 1.0 to 2.0, present more challenges.</p><p>The Zalando Operator offers two upgrade methods:</p><ul><li>Upgrade via Cloning: This approach requires the new cluster to run a higher version than the source. It may involve significant downtime, as all write operations must stop, and Write-Ahead Log (WAL) files must be archived before cloning begins.</li><li>In-place Major Version Upgrade: While more convenient, this method is irreversible once the <code>pg_upgrade</code> command is executed.</li></ul><p>The Zalando Postgres Operator primarily focuses on upgrades but may involve significant downtime. The search for a more flexible solution, especially with downgrade support and reduced downtime, remains ongoing.</p><h2 id=conventional-strategy>Conventional Strategy<a hidden class=anchor aria-hidden=true href=#conventional-strategy>#</a></h2><p>The conventional strategy involves the following steps:</p><ol><li>Stop the service to halt user requests.</li><li>Back up the database to be upgraded.</li><li>Set up the target version database and migrate the backup data.</li><li>Verify data consistency between the original and target databases.</li><li>If verification succeeds, resume the service and accept user requests.</li></ol><center><img style="zoom:50%;border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" src=https://github.com/user-attachments/assets/500388a6-837c-4565-9b8f-277176392d73><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px;margin-top:2px">Conventional Strategy</div></center><p>While this approach effectively handles database version changes, it requires system downtime, which depends on the speed of data migration and validation—the faster these processes, the shorter the downtime.</p><p>However, modern businesses increasingly demand high availability alongside system stability. For example, achieving &ldquo;five nines&rdquo; (99.999% uptime) requires minimal downtime, as shown in table below. The conventional strategy falls short of meeting these strict requirements, highlighting the need for alternative solutions that mitigate downtime and maintain high availability during database upgrades.</p><table><thead><tr><th style=text-align:left><strong>Availability %</strong></th><th style=text-align:left><strong>Downtime per year</strong></th><th style=text-align:left><strong>Downtime per day (24 hours)</strong></th></tr></thead><tbody><tr><td style=text-align:left>99%</td><td style=text-align:left>3.65 days</td><td style=text-align:left>14.40 minutes</td></tr><tr><td style=text-align:left>99.99%</td><td style=text-align:left>52.6 minutes</td><td style=text-align:left>8.64 seconds</td></tr><tr><td style=text-align:left>99.999%</td><td style=text-align:left>5.26 minutes</td><td style=text-align:left>864 milliseconds</td></tr></tbody></table><h2 id=our-solution>Our Solution<a hidden class=anchor aria-hidden=true href=#our-solution>#</a></h2><p>To address the challenges of database version changes and meet system design goals, the blue-green deployment strategy offers a practical solution.</p><p>The blue-green deployment strategy uses two parallel environments, &ldquo;blue&rdquo; and &ldquo;green&rdquo;. Each environment hosts a different database version, enabling smooth transitions between versions with near-zero downtime. The key benefit is continuous system operation during the version change, significantly reducing downtime.</p><p>The strategy involves four main steps as shown in the below image:</p><ul><li><p>Preparation: Set up both environments—blue hosting the current version and green the target version. The green environment is tested to ensure readiness for the transition.</p></li><li><p>Data Synchronization: Synchronize data between the blue and green environments to ensure consistency. This process is automated, reducing manual effort and enhancing efficiency and reliability.</p></li><li><p>Switching: After synchronization, the system switches from the blue to the green environment seamlessly, ensuring no service disruption, thus maintaining client transparency.</p></li><li><p>Monitoring: Post-switch, the new database version is monitored for performance and consistency. If issues arise, the system can quickly revert to the blue environment to minimize client impact.</p></li></ul><center><img style="zoom:50%;border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" src=https://github.com/user-attachments/assets/2b76a02f-1c26-417f-8699-fd7e6f2cb45d><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px;margin-top:2px">Blue-Green Strategy</div></center><p>By using the blue-green deployment strategy, we can efficiently manage database version changes, automate processes, and maintain client transparency, overcoming the limitations of conventional methods and better supporting customers&rsquo; needs.</p><h3 id=achieving-synchronisation-between-the-two-postgresql-databases>Achieving Synchronisation between The Two PostgreSQL Databases<a hidden class=anchor aria-hidden=true href=#achieving-synchronisation-between-the-two-postgresql-databases>#</a></h3><p>In the blue-green deployment strategy, synchronizing the two PostgreSQL databases is essential for maintaining data consistency and ensuring client transparency during version changes. We use logical replication as the primary method for database synchronization. It allows for selective, real-time data replication from one database to another. It ensures that changes in the source database are immediately reflected in the target database. This method is ideal for blue-green deployments, enabling smooth transitions between database versions with near-zero downtime and client disruption.</p><h3 id=monitoring-synchronisation-progress>Monitoring Synchronisation Progress<a hidden class=anchor aria-hidden=true href=#monitoring-synchronisation-progress>#</a></h3><p>A critical factor in ensuring a smooth database version change is monitoring the synchronization progress between the master and follower nodes. In PostgreSQL, the <code>pg_stat_replication</code> view provides key replication status metrics to track synchronization completion.</p><p>The following metrics from the <code>pg_stat_replication</code> view are essential for monitoring synchronization progress:</p><ul><li><code>sent_lsn</code>: Represents the latest Write-Ahead Log (WAL) position sent by the master to the follower.</li><li><code>pg_current_wal_flush_lsn</code>: Returns the latest WAL position flushed to disk on the master.</li><li><code>pg_wal_lsn_diff(lsn1, lsn2)</code>: Calculates the byte difference between two WAL positions.</li></ul><h4 id=determining-synchronisation-completion>Determining Synchronisation Completion<a hidden class=anchor aria-hidden=true href=#determining-synchronisation-completion>#</a></h4><p>To determine when synchronization is complete, we compare the <code>sent_lsn</code> value with the <code>pg_current_wal_flush_lsn</code> value. When they match, it indicates that the latest WAL position sent by the master has been flushed to disk, marking the completion of synchronization.</p><p>Additionally, the <code>pg_wal_lsn_diff</code> function can track the remaining byte difference between the master and follower WAL positions, helping estimate the remaining synchronization time for more accurate system adjustments.</p><p>By utilizing the <code>pg_stat_replication</code> view and associated functions, we can effectively monitor synchronization progress, ensuring near-zero downtime, automation, and transparent operation during database version changes.</p><p>In practice, since clients may continuously write to the database, <code>sent_lsn</code> may never fully catch up with <code>pg_current_wal_flush_lsn</code>. However, the difference is typically small. To ensure synchronization, we temporarily halt write operations and wait for the values to match. This is why we refer to it as &ldquo;near-zero&rdquo; downtime rather than &ldquo;zero&rdquo; downtime.</p><h3 id=integrating-blue-green-deployment-strategy-with-kubernetes>Integrating Blue-Green Deployment Strategy with Kubernetes<a hidden class=anchor aria-hidden=true href=#integrating-blue-green-deployment-strategy-with-kubernetes>#</a></h3><p>Custom Resource Definitions (CRDs) extend Kubernetes API, allowing users to define and manage custom resources.</p><p>To design the CRD, we define a schema with the following key fields:</p><ul><li><code>apiVersion</code>: The API version used for the CRD.</li><li><code>kind</code>: The type of Kubernetes object, in this case, a custom resource for blue-green deployment.</li><li><code>metadata</code>: Includes details like name, namespace, labels, and annotations.</li><li><code>spec</code>: Specifies the properties of the custom resource.</li></ul><h2 id=implementation>Implementation<a hidden class=anchor aria-hidden=true href=#implementation>#</a></h2><p>We begin by discussing the database synchronization process and the configuration of master and follower nodes. The section then introduces the <code>PgUpgradeReconciler</code> structure and its integration with Kubernetes. The core of the implementation is the reconciliation process, which includes controller setup, deployment creation, service creation, schema synchronization, subscription management, Nginx proxy updates, resource cleanup, and supporting functions. Each step is carefully detailed to ensure smooth integration of the Blue-Green deployment strategy with Kubernetes, enabling database version changes with minimal downtime. This section serves as a practical guide to implementing this solution in real-world scenarios.</p><h3 id=synchronisation-between-databases>Synchronisation between Databases<a hidden class=anchor aria-hidden=true href=#synchronisation-between-databases>#</a></h3><p>Both the master and follower nodes require specific configurations.</p><h4 id=master-node-configuration>Master Node Configuration<a hidden class=anchor aria-hidden=true href=#master-node-configuration>#</a></h4><p>The Kubernetes Operator automates these tasks to set up logical replication on the master node:</p><ol><li>Configure PostgreSQL: Updates <code>postgresql.conf</code> to enable logical replication by setting <code>wal_level</code> to logical.</li><li>Create Publication: Uses the SQL command <code>CREATE PUBLICATION</code> to create a publication that includes all tables.</li><li>Export Schema: Runs the <code>pg_dump</code> command to export the database schema to an SQL file.</li></ol><h4 id=follower-node-configuration>Follower Node Configuration<a hidden class=anchor aria-hidden=true href=#follower-node-configuration>#</a></h4><p>The Kubernetes Operator automates schema synchronization and subscription creation on the follower node:</p><ol><li>Synchronize Schema: Imports the schema from the master using the <code>psql</code> command.</li><li>Create Subscription: Uses the SQL command <code>CREATE SUBSCRIPTION</code> to establish a subscription to the master node’s publication.</li></ol><h3 id=pgupgradereconciler-structure><code>PgUpgradeReconciler</code> Structure<a hidden class=anchor aria-hidden=true href=#pgupgradereconciler-structure>#</a></h3><p>The <code>PgUpgradeReconciler</code> struct includes the following fields:</p><ol><li><code>client.Client</code>: Manages CRUD operations on Kubernetes objects.</li><li>Scheme: A <code>runtime.Scheme</code> for converting between Go structs and GroupVersionKinds.</li><li>Log: A <code>logr.Logger</code> instance for logging messages at various verbosity levels.</li></ol><h3 id=kubernetes-rbac>Kubernetes RBAC<a hidden class=anchor aria-hidden=true href=#kubernetes-rbac>#</a></h3><p>Kubebuilder annotations define the required RBAC permissions for the controller to manage resources:</p><ol><li><code>pgupgrades</code>: Custom resource for managing PostgreSQL upgrades.</li><li><code>pgupgrades/status</code>: Subresource for handling <code>PgUpgrade</code> object statuses.</li><li><code>pgupgrades/finalizers</code>: Subresource for managing finalizers on <code>PgUpgrade</code> objects.</li><li><code>pods</code>, <code>deployments</code>, <code>services</code>, <code>configmaps</code>: Standard Kubernetes resources for creating, managing, and deleting related resources.</li></ol><h3 id=reconciliation-process>Reconciliation Process<a hidden class=anchor aria-hidden=true href=#reconciliation-process>#</a></h3><p>The <code>Reconcile</code> function is the heart of the controller, managing the reconciliation process. It starts by retrieving the <code>PgUpgrade</code> instance. If the instance is not found, it returns a nil error, allowing the controller to proceed with other instances.</p><h4 id=controller-setup>Controller Setup<a hidden class=anchor aria-hidden=true href=#controller-setup>#</a></h4><p>The <code>PgUpgradeReconciler</code> struct defines the controller for the <code>PgUpgrade</code> system. The <code>SetupWithManager</code> function sets up the controller with the Manager, associating it with the <code>pgupgradev1.PgUpgrade</code> resource.</p><h4 id=deployment-creation>Deployment Creation<a hidden class=anchor aria-hidden=true href=#deployment-creation>#</a></h4><p>The function checks if the <code>PgUpgrade</code> deployment exists. If not, it creates one using the <code>deploymentForPgUpgrade</code> function and logs the process. If it exists, it checks if the image matches the one in the <code>PgUpgrade</code> object and updates it if needed.</p><h4 id=service-creation>Service Creation<a hidden class=anchor aria-hidden=true href=#service-creation>#</a></h4><p>The function checks if the <code>PgUpgrade</code> service exists. If not, it creates one using the <code>serviceForPgUpgrade</code> function and logs it. If the service already exists, it proceeds.</p><h4 id=schema-synchronization>Schema Synchronization<a hidden class=anchor aria-hidden=true href=#schema-synchronization>#</a></h4><p>The <code>syncSchema</code> function runs schema synchronization on the target PostgreSQL instance by connecting to the target pod, retrieving its IP, and executing the schema sync command via the <code>remotecommand</code> package.</p><h4 id=subscription-creation>Subscription Creation<a hidden class=anchor aria-hidden=true href=#subscription-creation>#</a></h4><p>The <code>createSubscriptions</code> function sets up logical replication between the old and new PostgreSQL instances by connecting to the new instance and running the <code>CREATE SUBSCRIPTION</code> command with the necessary parameters.</p><h4 id=nginx-proxy-update>Nginx Proxy Update<a hidden class=anchor aria-hidden=true href=#nginx-proxy-update>#</a></h4><p>The <code>changeNginxProxyPass</code> function updates the Nginx configuration stored in a <code>ConfigMap</code>. It retrieves the <code>ConfigMap</code>, modifies the <code>proxy_pass</code> directive to point to the new follower’s IP, and updates the <code>ConfigMap</code> in the cluster.</p><h4 id=resource-deletion>Resource Deletion<a hidden class=anchor aria-hidden=true href=#resource-deletion>#</a></h4><p>The <code>deleteResource</code> function removes specified resources (e.g., deployments) from the cluster. It iterates over the resource names in the <code>KillDeployments</code> field of the <code>PgUpgrade</code> resource and deletes them, logging the process.</p><h4 id=helper-functions>Helper Functions<a hidden class=anchor aria-hidden=true href=#helper-functions>#</a></h4><p>Various helper functions support the system, including:</p><ol><li><code>labelsForPgUpgrade</code>: Generates labels for a <code>PgUpgrade</code> resource.</li><li><code>getFollowerIP</code>: Retrieves the follower&rsquo;s IP from the cluster by fetching the pod&rsquo;s IP based on the <code>OldDBLabel</code>.</li></ol><h2 id=prototype-outcomes>Prototype Outcomes<a hidden class=anchor aria-hidden=true href=#prototype-outcomes>#</a></h2><p>The demo video above clearly shows the process and the outcome of this project. Here I would like to discuss the performance of our solution.</p><h3 id=performance-analysis>Performance Analysis<a hidden class=anchor aria-hidden=true href=#performance-analysis>#</a></h3><p>We use <code>PgAdmin4</code> to analyze the performance metrics of the database within the Kubernetes cluster. These metrics include database sessions, transactions per second, tuples in/out, and block I/O.</p><p>Figure PA-1 shows the baseline database performance, with stable database sessions and tuples in. Transactions per second, tuples out, and block I/O follow a consistent and regular pattern.</p><center><img style="zoom:50%;border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" src=https://github.com/user-attachments/assets/660fe93e-9780-44dc-ba41-7cea7b5c7d11><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px;margin-top:2px">PA-1 The initial performance metrics.</div></center><p>Figure PA-2 depicts the system’s behavior after starting the client program, which runs five goroutines. Each goroutine reads from the database every three seconds and writes every ten seconds, reflecting the typical pattern of more frequent reads than writes. This activity causes a noticeable increase in all metrics, but the system remains stable.</p><center><img style="zoom:50%;border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" src=https://github.com/user-attachments/assets/72e3ddf5-6ac5-4012-a22d-70d1d293a575><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px;margin-top:2px">PA-2 The performance metrics after running the client.</div></center><p>Figure PA-3 illustrates the system’s response when our Kubernetes operator initiates a database version change. Aside from a minor increase in the &ldquo;tuples in&rdquo; metric (due to client operations), a brief fluctuation in metrics is observed, but the system quickly returns to normal. These rapid changes, highlighted by red boxes, are expected as the version change involves data synchronization between the master and follower nodes. Importantly, no idle sessions or transactions occur during this process, demonstrating that our solution achieves near-zero downtime while remaining transparent to clients.</p><center><img style="zoom:50%;border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" src=https://github.com/user-attachments/assets/e2a794ac-4463-4e13-b1cd-bb42aec19c5f><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px;margin-top:2px">PA-3 The performance metrics after deploying the Kubernetes operator.</div></center><p>The results confirm that our solution effectively handles database version migrations and traffic switching. The smooth user experience during the migration highlights the approach&rsquo;s practicality and reliability.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://zzheng2020.github.io/tags/kubernetes/>Kubernetes</a></li><li><a href=https://zzheng2020.github.io/tags/docker/>Docker</a></li><li><a href=https://zzheng2020.github.io/tags/golang/>Golang</a></li></ul><nav class=paginav><a class=prev href=https://zzheng2020.github.io/post/lockfree/><span class=title>« Prev</span><br><span>Lock Free Queue And Time Wheel Mechanism</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Master Thesis: High Availability in Lifecycle Management of Cloud-Native Network Functions on x" href="https://x.com/intent/tweet/?text=Master%20Thesis%3a%20High%20Availability%20in%20Lifecycle%20Management%20of%20Cloud-Native%20Network%20Functions&amp;url=https%3a%2f%2fzzheng2020.github.io%2fpost%2fmasterthesis%2f&amp;hashtags=Kubernetes%2cDocker%2cGolang"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Master Thesis: High Availability in Lifecycle Management of Cloud-Native Network Functions on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fzzheng2020.github.io%2fpost%2fmasterthesis%2f&amp;title=Master%20Thesis%3a%20High%20Availability%20in%20Lifecycle%20Management%20of%20Cloud-Native%20Network%20Functions&amp;summary=Master%20Thesis%3a%20High%20Availability%20in%20Lifecycle%20Management%20of%20Cloud-Native%20Network%20Functions&amp;source=https%3a%2f%2fzzheng2020.github.io%2fpost%2fmasterthesis%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Master Thesis: High Availability in Lifecycle Management of Cloud-Native Network Functions on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fzzheng2020.github.io%2fpost%2fmasterthesis%2f&title=Master%20Thesis%3a%20High%20Availability%20in%20Lifecycle%20Management%20of%20Cloud-Native%20Network%20Functions"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Master Thesis: High Availability in Lifecycle Management of Cloud-Native Network Functions on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fzzheng2020.github.io%2fpost%2fmasterthesis%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Master Thesis: High Availability in Lifecycle Management of Cloud-Native Network Functions on whatsapp" href="https://api.whatsapp.com/send?text=Master%20Thesis%3a%20High%20Availability%20in%20Lifecycle%20Management%20of%20Cloud-Native%20Network%20Functions%20-%20https%3a%2f%2fzzheng2020.github.io%2fpost%2fmasterthesis%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Master Thesis: High Availability in Lifecycle Management of Cloud-Native Network Functions on telegram" href="https://telegram.me/share/url?text=Master%20Thesis%3a%20High%20Availability%20in%20Lifecycle%20Management%20of%20Cloud-Native%20Network%20Functions&amp;url=https%3a%2f%2fzzheng2020.github.io%2fpost%2fmasterthesis%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Master Thesis: High Availability in Lifecycle Management of Cloud-Native Network Functions on ycombinator" href="https://news.ycombinator.com/submitlink?t=Master%20Thesis%3a%20High%20Availability%20in%20Lifecycle%20Management%20of%20Cloud-Native%20Network%20Functions&u=https%3a%2f%2fzzheng2020.github.io%2fpost%2fmasterthesis%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://zzheng2020.github.io/>Ziheng Zhang's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>