[{"content":"Once upon a time, in a forest where whispers traveled faster than the wind, there lived a sparrow. He was lighthearted and swift, often flitting from branch to branch, curious about the world but careful not to settle down too soon. One fateful morning, he met a magpie with feathers as dark as night and eyes that gleamed like stolen stars. She sang a song so captivating that, for a moment, the sparrow stopped to listen.\n“You and I,” said the magpie, “should share a nest. Together, we could craft a melody that echoes across the woods.”\nThe sparrow, entranced by the thought, agreed without hesitation. But as the day wore on, he realised the magpie’s melody was not quite what he imagined. Her notes clashed with his rhythm, her perch too sharp for his tender claws. By sunset, the sparrow knew he had been too hasty.\n“I am sorry,” he told the magpie. “I spoke too quickly. I don’t think we are meant to sing together.”\nBut the magpie’s eyes flashed, her wings trembling with rage. “You dare to fly away?” she screeched. “You promised a duet!”\nThe sparrow tried to explain, but the magpie was not one to listen. In her fury, she soared to the tallest tree in the forest, calling out to all the creatures below. “Hear me, everyone! This sparrow is a traitor! A liar! He led me to believe we’d build a future, only to abandon me!” And with that, she scattered the sparrow’s feathers—pieces of his name, his story, his very essence—into the winds.\nAt first, the sparrow was stunned. He perched on a low branch, watching as squirrels chattered, foxes whispered, and owls hooted, their eyes glinting with curiosity. But then, something happened. The creatures of the forest, wise from seasons of rumour and deceit, began to see through the magpie’s game.\n“Wait,” said the owl, “didn’t she once try to stir up drama about the jay in the group chat?”\n“And the robin!” added a chipmunk. “Remember how she even sent shiny coins to get everyone to pay attention, but no one cared?”\nThe creatures began to laugh, their chatter turning against the magpie. Her plan to humiliate the sparrow had backfired. She fluttered away, defeated, while the sparrow, though shaken, remained untouched.\nAs he flew back to his favourite branch, the sparrow reflected on what had happened. His feathers were ruffled, his song quieter, but he realised the magpie’s storm had passed without leaving scars. He thought of the creatures who had defended him, who saw through the magpie’s rage, and felt a warmth in his chest.\n“Sometimes,” he thought, “the truth doesn’t need to be shouted. It just needs to be seen.”\nAnd so, the sparrow sang again—not for the magpie, nor for the crowd, but for himself. His notes were steady, his rhythm true. And though the magpie’s cries echoed faintly in the distance, they no longer mattered.\nThe forest, after all, had already made its judgment.\n","permalink":"https://zzheng2020.github.io/post/the_tale_of_the_hasty_sparrow_and_the_angry_magpie/","summary":"When the storm passes, clarity remains. Here’s a story of haste, chaos, and the surprising strength of truth.","title":"The Tale of the Hasty Sparrow and the Angry Magpie"},{"content":"Lock-free Queue In multithreaded programming, queues are frequently used, especially in producer-consumer models. However, using locks (e.g., std::mutex) to manage shared queues can degrade performance. In high-concurrency scenarios, lock contention leads to frequent context switching, reducing efficiency. A lock-free queue provides a solution by using atomic operations to ensure thread safety without locks.\nHere I am going to show how to implement a thread-safe and lock-free queue using std::atomic and a ring buffer in C++ to improve performance (when crital section is relatively small).\nData Structure for Lock-free Queue LockFreeQueue uses std::atomic to manage producer and consumer states, while using a ring buffer for data storage to avoid frequent memory allocations. To enable circular buffer, it uses modulo, ensuring pointers wrap around without exceeding boundaries, thereby eliminating the overhead of dynamic resizing.\nprivate: std::atomic\u0026lt;int\u0026gt; producerHead_; std::atomic\u0026lt;int\u0026gt; producerTail_; std::atomic\u0026lt;int\u0026gt; consumerHead_; std::vector\u0026lt;T\u0026gt; data_{SIZE}; inline int getIndex(int t) const { return t % SIZE; } Push Operation The main logic of the push operation is to update producerHead_ through the fetch_add atomic operation and ensure that the space that the consumer has not read can be written to the data.\nstd::optional\u0026lt;int\u0026gt; push(const T\u0026amp; elem) { int p_head = producerHead_.fetch_add(1, std::memory_order_seq_cst); int p_next = p_head + 1; if (consumerHead_.load(std::memory_order_seq_cst) == p_next) { // The queue is full. return std::nullopt; } data_[getIndex(p_head)] = std::move(elem); // Wait other threads to finish the push operation. int wait_cycles = 0; while (producerTail_.load(std::memory_order_seq_cst) != p_head) { if (wait_cycles % 1000 == 0) { std::this_thread::yield(); } wait_cycles++; } producerTail_.store(p_next, std::memory_order_seq_cst); return getIndex(p_head); } The queue is full by checking if consumerHead_ is equal to p_next. If the consumer\u0026rsquo;s read pointer catches up with the producer\u0026rsquo;s write pointer, the queue is full and no more data can be written.\nPop Operation When dequeuing, the consumer reads the data from the location pointed to by consumerHead_ and advances the pointer.\nbool pop(T\u0026amp; elem) { int c_head = consumerHead_.load(std::memory_order_seq_cst); int c_next = c_head + 1; if (producerTail_.load(std::memory_order_seq_cst) == c_head) { return false; // The queue is empty. } elem = std::move(data_[getIndex(c_head)]); consumerHead_.store(c_next, std::memory_order_seq_cst); return true; } Time Wheel Algorithm The time wheel is a ring-like structure that divides time into multiple slots (MAX_TIMEOUT_VALUE indicates the number of slots), each of which can hold a timer. The time wheel \u0026ldquo;turns\u0026rdquo; forward one space after each time period. Whenever the time wheel pointer points to a slot, it checks whether there is an expired timer in the slot. If there is an expired timer, the corresponding callback function is executed.\nData Structure for Time Wheel private: std::atomic\u0026lt;uint32_t\u0026gt; now_; Callback callback_; std::vector\u0026lt;LockFreeQueue\u0026lt;MAX_TIMEOUT_VALUE, uint32_t\u0026gt;\u0026gt; timers_; now_: Current time, which is accessed safely in a multi-threaded environment using the atomic type std::atomic. Each time tick is called, now_ is incremented to advance the time wheel.\ncallback_: This is the callback function that needs to be called when the timer times out.\ntimers: Each element is a queue used to store the timer ID.\nSet Function The timer is set through the set method. The timer is assigned to a slot according to its timeout period and stored in the lock-free queue timers_[].\nvoid set(uint32_t id, uint64_t timeout) { assert(timeout \u0026gt; 0); assert(timeout \u0026lt; MAX_TIMEOUT_VALUE); uint32_t targetTime = (now_ + timeout) % MAX_TIMEOUT_VALUE; timers_[targetTime].push(id); } Tick Function The tick method is used to advance the time wheel. When the time wheel advances to a certain slot, it checks the timer task in the slot and triggers the callback function.\nvoid tick(uint32_t milliseconds = 1) { for (uint32_t ms = 0; ms \u0026lt; milliseconds; ++ms) { now_ = (now_ + 1) % MAX_TIMEOUT_VALUE; uint32_t id; while (timers_[now_].pop(id)) { callback_(id); } } } ","permalink":"https://zzheng2020.github.io/post/lockfree/","summary":"A C++ Implementation","title":"Lock Free Queue And Time Wheel Mechanism"},{"content":"This blog introduces my master\u0026rsquo;s thesis ([pdf] [presentation] [code]) and the demo video can be found below. Most of the sections correspond to paragraphs from the thesis, while some parts that are necessary in the thesis but not needed in the blog have been omitted. This allows you to focus more on the project itself.\nBackground In any infrastructure, be it clustered or otherwise, databases often face challenges with version changes, including upgrades and downgrades. Upgrades are straightforward, driven by the need to adopt new features or improvements. However, downgrades, especially in the telecommunications industry, present unique challenges. For example, in the Access and Mobility Management Function (AMF) as shown in the image below, when a database is upgraded and data is converted to a new version, complications may arise. According to ETSI MANO specifications, a rollback to the previous version may be required in such cases.\n3GPP 5G Core Architecture Another scenario is the \u0026ldquo;observation period\u0026rdquo; following an upgrade. For example, after upgrading a service from version A to version B, its performance is monitored. Key questions arise: Is the service functioning correctly? Is it handling the expected load? If the service underperforms, a rollback may be required, presenting the same challenges as in the first scenario. Both cases highlight the need for better support for database downgrades to mitigate service disruptions and maintain user experience.\nKubernetes Operator A Kubernetes Operator (K8s Operator) encodes human operational expertise into software. The image below illustrates the key differences between using a K8s Operator and not.\nComparison of Cluster Management: Manual vs. K8s Operator Without a K8s Operator, maintainers must deeply understand how stateful applications work within the cluster, execute commands in the correct order, and manually verify responses.\nWith a K8s Operator, maintainers can perform complex operations, like database upgrades or downgrades, by simply modifying the custom resource definition (YAML file), without needing to understand the internal mechanisms.\nIn these scenarios, the K8s Operator acts like an experienced maintainer, monitoring the cluster 24/7 and exposing a simple API, greatly reducing the chance of human errors.\nZalando Postgres Operator The Zalando Postgres Operator is a widely-used tool in the industry, known for supporting both minor and major version upgrades.\nMinor upgrades, such as moving from version 1.0 to 1.1, are straightforward and can be performed without downtime. However, major upgrades, like transitioning from version 1.0 to 2.0, present more challenges.\nThe Zalando Operator offers two upgrade methods:\nUpgrade via Cloning: This approach requires the new cluster to run a higher version than the source. It may involve significant downtime, as all write operations must stop, and Write-Ahead Log (WAL) files must be archived before cloning begins. In-place Major Version Upgrade: While more convenient, this method is irreversible once the pg_upgrade command is executed. The Zalando Postgres Operator primarily focuses on upgrades but may involve significant downtime. The search for a more flexible solution, especially with downgrade support and reduced downtime, remains ongoing.\nConventional Strategy The conventional strategy involves the following steps:\nStop the service to halt user requests. Back up the database to be upgraded. Set up the target version database and migrate the backup data. Verify data consistency between the original and target databases. If verification succeeds, resume the service and accept user requests. Conventional Strategy While this approach effectively handles database version changes, it requires system downtime, which depends on the speed of data migration and validation—the faster these processes, the shorter the downtime.\nHowever, modern businesses increasingly demand high availability alongside system stability. For example, achieving \u0026ldquo;five nines\u0026rdquo; (99.999% uptime) requires minimal downtime, as shown in table below. The conventional strategy falls short of meeting these strict requirements, highlighting the need for alternative solutions that mitigate downtime and maintain high availability during database upgrades.\nAvailability % Downtime per year Downtime per day (24 hours) 99% 3.65 days 14.40 minutes 99.99% 52.6 minutes 8.64 seconds 99.999% 5.26 minutes 864 milliseconds Our Solution To address the challenges of database version changes and meet system design goals, the blue-green deployment strategy offers a practical solution.\nThe blue-green deployment strategy uses two parallel environments, \u0026ldquo;blue\u0026rdquo; and \u0026ldquo;green\u0026rdquo;. Each environment hosts a different database version, enabling smooth transitions between versions with near-zero downtime. The key benefit is continuous system operation during the version change, significantly reducing downtime.\nThe strategy involves four main steps as shown in the below image:\nPreparation: Set up both environments—blue hosting the current version and green the target version. The green environment is tested to ensure readiness for the transition.\nData Synchronization: Synchronize data between the blue and green environments to ensure consistency. This process is automated, reducing manual effort and enhancing efficiency and reliability.\nSwitching: After synchronization, the system switches from the blue to the green environment seamlessly, ensuring no service disruption, thus maintaining client transparency.\nMonitoring: Post-switch, the new database version is monitored for performance and consistency. If issues arise, the system can quickly revert to the blue environment to minimize client impact.\nBlue-Green Strategy By using the blue-green deployment strategy, we can efficiently manage database version changes, automate processes, and maintain client transparency, overcoming the limitations of conventional methods and better supporting customers\u0026rsquo; needs.\nAchieving Synchronisation between The Two PostgreSQL Databases In the blue-green deployment strategy, synchronizing the two PostgreSQL databases is essential for maintaining data consistency and ensuring client transparency during version changes. We use logical replication as the primary method for database synchronization. It allows for selective, real-time data replication from one database to another. It ensures that changes in the source database are immediately reflected in the target database. This method is ideal for blue-green deployments, enabling smooth transitions between database versions with near-zero downtime and client disruption.\nMonitoring Synchronisation Progress A critical factor in ensuring a smooth database version change is monitoring the synchronization progress between the master and follower nodes. In PostgreSQL, the pg_stat_replication view provides key replication status metrics to track synchronization completion.\nThe following metrics from the pg_stat_replication view are essential for monitoring synchronization progress:\nsent_lsn: Represents the latest Write-Ahead Log (WAL) position sent by the master to the follower. pg_current_wal_flush_lsn: Returns the latest WAL position flushed to disk on the master. pg_wal_lsn_diff(lsn1, lsn2): Calculates the byte difference between two WAL positions. Determining Synchronisation Completion To determine when synchronization is complete, we compare the sent_lsn value with the pg_current_wal_flush_lsn value. When they match, it indicates that the latest WAL position sent by the master has been flushed to disk, marking the completion of synchronization.\nAdditionally, the pg_wal_lsn_diff function can track the remaining byte difference between the master and follower WAL positions, helping estimate the remaining synchronization time for more accurate system adjustments.\nBy utilizing the pg_stat_replication view and associated functions, we can effectively monitor synchronization progress, ensuring near-zero downtime, automation, and transparent operation during database version changes.\nIn practice, since clients may continuously write to the database, sent_lsn may never fully catch up with pg_current_wal_flush_lsn. However, the difference is typically small. To ensure synchronization, we temporarily halt write operations and wait for the values to match. This is why we refer to it as \u0026ldquo;near-zero\u0026rdquo; downtime rather than \u0026ldquo;zero\u0026rdquo; downtime.\nIntegrating Blue-Green Deployment Strategy with Kubernetes Custom Resource Definitions (CRDs) extend Kubernetes API, allowing users to define and manage custom resources.\nTo design the CRD, we define a schema with the following key fields:\napiVersion: The API version used for the CRD. kind: The type of Kubernetes object, in this case, a custom resource for blue-green deployment. metadata: Includes details like name, namespace, labels, and annotations. spec: Specifies the properties of the custom resource. Implementation We begin by discussing the database synchronization process and the configuration of master and follower nodes. The section then introduces the PgUpgradeReconciler structure and its integration with Kubernetes. The core of the implementation is the reconciliation process, which includes controller setup, deployment creation, service creation, schema synchronization, subscription management, Nginx proxy updates, resource cleanup, and supporting functions. Each step is carefully detailed to ensure smooth integration of the Blue-Green deployment strategy with Kubernetes, enabling database version changes with minimal downtime. This section serves as a practical guide to implementing this solution in real-world scenarios.\nSynchronisation between Databases Both the master and follower nodes require specific configurations.\nMaster Node Configuration The Kubernetes Operator automates these tasks to set up logical replication on the master node:\nConfigure PostgreSQL: Updates postgresql.conf to enable logical replication by setting wal_level to logical. Create Publication: Uses the SQL command CREATE PUBLICATION to create a publication that includes all tables. Export Schema: Runs the pg_dump command to export the database schema to an SQL file. Follower Node Configuration The Kubernetes Operator automates schema synchronization and subscription creation on the follower node:\nSynchronize Schema: Imports the schema from the master using the psql command. Create Subscription: Uses the SQL command CREATE SUBSCRIPTION to establish a subscription to the master node’s publication. PgUpgradeReconciler Structure The PgUpgradeReconciler struct includes the following fields:\nclient.Client: Manages CRUD operations on Kubernetes objects. Scheme: A runtime.Scheme for converting between Go structs and GroupVersionKinds. Log: A logr.Logger instance for logging messages at various verbosity levels. Kubernetes RBAC Kubebuilder annotations define the required RBAC permissions for the controller to manage resources:\npgupgrades: Custom resource for managing PostgreSQL upgrades. pgupgrades/status: Subresource for handling PgUpgrade object statuses. pgupgrades/finalizers: Subresource for managing finalizers on PgUpgrade objects. pods, deployments, services, configmaps: Standard Kubernetes resources for creating, managing, and deleting related resources. Reconciliation Process The Reconcile function is the heart of the controller, managing the reconciliation process. It starts by retrieving the PgUpgrade instance. If the instance is not found, it returns a nil error, allowing the controller to proceed with other instances.\nController Setup The PgUpgradeReconciler struct defines the controller for the PgUpgrade system. The SetupWithManager function sets up the controller with the Manager, associating it with the pgupgradev1.PgUpgrade resource.\nDeployment Creation The function checks if the PgUpgrade deployment exists. If not, it creates one using the deploymentForPgUpgrade function and logs the process. If it exists, it checks if the image matches the one in the PgUpgrade object and updates it if needed.\nService Creation The function checks if the PgUpgrade service exists. If not, it creates one using the serviceForPgUpgrade function and logs it. If the service already exists, it proceeds.\nSchema Synchronization The syncSchema function runs schema synchronization on the target PostgreSQL instance by connecting to the target pod, retrieving its IP, and executing the schema sync command via the remotecommand package.\nSubscription Creation The createSubscriptions function sets up logical replication between the old and new PostgreSQL instances by connecting to the new instance and running the CREATE SUBSCRIPTION command with the necessary parameters.\nNginx Proxy Update The changeNginxProxyPass function updates the Nginx configuration stored in a ConfigMap. It retrieves the ConfigMap, modifies the proxy_pass directive to point to the new follower’s IP, and updates the ConfigMap in the cluster.\nResource Deletion The deleteResource function removes specified resources (e.g., deployments) from the cluster. It iterates over the resource names in the KillDeployments field of the PgUpgrade resource and deletes them, logging the process.\nHelper Functions Various helper functions support the system, including:\nlabelsForPgUpgrade: Generates labels for a PgUpgrade resource. getFollowerIP: Retrieves the follower\u0026rsquo;s IP from the cluster by fetching the pod\u0026rsquo;s IP based on the OldDBLabel. Prototype Outcomes The demo video above clearly shows the process and the outcome of this project. Here I would like to discuss the performance of our solution.\nPerformance Analysis We use PgAdmin4 to analyze the performance metrics of the database within the Kubernetes cluster. These metrics include database sessions, transactions per second, tuples in/out, and block I/O.\nFigure PA-1 shows the baseline database performance, with stable database sessions and tuples in. Transactions per second, tuples out, and block I/O follow a consistent and regular pattern.\nPA-1 The initial performance metrics. Figure PA-2 depicts the system’s behavior after starting the client program, which runs five goroutines. Each goroutine reads from the database every three seconds and writes every ten seconds, reflecting the typical pattern of more frequent reads than writes. This activity causes a noticeable increase in all metrics, but the system remains stable.\nPA-2 The performance metrics after running the client. Figure PA-3 illustrates the system’s response when our Kubernetes operator initiates a database version change. Aside from a minor increase in the \u0026ldquo;tuples in\u0026rdquo; metric (due to client operations), a brief fluctuation in metrics is observed, but the system quickly returns to normal. These rapid changes, highlighted by red boxes, are expected as the version change involves data synchronization between the master and follower nodes. Importantly, no idle sessions or transactions occur during this process, demonstrating that our solution achieves near-zero downtime while remaining transparent to clients.\nPA-3 The performance metrics after deploying the Kubernetes operator. The results confirm that our solution effectively handles database version migrations and traffic switching. The smooth user experience during the migration highlights the approach\u0026rsquo;s practicality and reliability.\n","permalink":"https://zzheng2020.github.io/post/masterthesis/","summary":"A Near-Zero Downtime Database Version Change Prototype","title":"Master Thesis: High Availability in Lifecycle Management of Cloud-Native Network Functions"}]